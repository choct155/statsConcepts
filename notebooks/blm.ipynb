{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ddd538",
   "metadata": {},
   "source": [
    "# Bayesian Linear Modeling\n",
    "\n",
    "Bayesian approaches to estimation offer a great deal of flexibility in modeling a given data generating process. However, few introductions to econometrics offer a bayesian perspective at the outset. This creates an unfortunate gap in understanding the ways in which bayesian and frequentist approaches are related. Relatedly, there is often another gap in understanding how bayesian approaches are actually operationalized. \n",
    "\n",
    "The purpose of this notebook is to demonstrate that bayesian approaches to linear regression can service the same needs as canonical frequentist approaches. They also buy us ways of directly modeling uncertainty. To show this, we'll use the `mpg` dataset (provided via `seaborn`) to explore the relationship between miles per gallon and vehicle weight. Specifically, we will estimate that relationship using Ordinary Least Squares regression in three ways:\n",
    "\n",
    "1. OLS analytically derived from the [normal equation](https://mathworld.wolfram.com/NormalEquation.html)\n",
    "2. OLS estimated by maximum likelihood\n",
    "3. OLS estimated via posterior sampling in bayesian linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a20242a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'naming' from 'tensorflow.python.autograph.core' (/usr/local/Caskroom/miniconda/base/envs/tf/lib/python3.7/site-packages/tensorflow/python/autograph/core/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f67afeea3326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/tf/lib/python3.7/site-packages/tensorflow_probability/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# pylint: enable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/tf/lib/python3.7/site-packages/tensorflow_probability/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/tf/lib/python3.7/site-packages/tensorflow_probability/python/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauto_batching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0medward2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmcmc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/tf/lib/python3.7/site-packages/tensorflow_probability/python/experimental/auto_batching/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_batching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mallocation_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_batching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdsl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_batching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrontend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_batching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minstructions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_batching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mliveness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/tf/lib/python3.7/site-packages/tensorflow_probability/python/experimental/auto_batching/frontend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreturn_statements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnaming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyct\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyct\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'naming' from 'tensorflow.python.autograph.core' (/usr/local/Caskroom/miniconda/base/envs/tf/lib/python3.7/site-packages/tensorflow/python/autograph/core/__init__.py)"
     ]
    }
   ],
   "source": [
    "import seaborn as sb\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.axes import Axes\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from statsconcepts.regression import OLS, RegModel\n",
    "from dataclasses import dataclass\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "colors: List[str] = sb.color_palette(\"Set1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a74fef8-1c69-46db-920b-76bd60a103e3",
   "metadata": {},
   "source": [
    "# Input Data\n",
    "\n",
    "The `mpg` dataset can be loaded directly from the `seaborn` package. Note that our goal is to be able to say things like when the weight of a car is $x$ pounds above average, we tend to see that miles per gallon is $y$ miles below average. To facilitate this framing, we are going to demean our variables of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66836473-662f-46ab-8149-b79233308fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds: xr.Dataset = xr.Dataset(sb.load_dataset(\"mpg\"))\n",
    "ds[\"demeaned_mpg\"] = ds[\"mpg\"] - ds[\"mpg\"].mean()\n",
    "ds[\"demeaned_weight\"] = ds[\"weight\"] - ds[\"weight\"].mean()\n",
    "\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2998188-ee46-44ed-bff1-e1f8dcf54b90",
   "metadata": {},
   "source": [
    "As can be seen in the plot below, lower weights are associated with higher miles per gallon and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cfca4-7d78-460d-9307-30b170c6c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(ds: xr.Dataset, x: str, y: str, figsize: Tuple[int, int] = (15, 10), **kwargs) -> Axes:\n",
    "    fig: Figure\n",
    "    ax: Axes\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sb.scatterplot(x=ds[x], y=ds[y], ax=ax, color=\"white\", ec=\"red\")\n",
    "    return ax\n",
    "    \n",
    "plot_data(ds, \"demeaned_weight\", \"demeaned_mpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4668f8f-9fc5-4092-90de-61350a09abdd",
   "metadata": {},
   "source": [
    "# Analytic Solution\n",
    "\n",
    "The first approach we typically learn is to directly use the regression equation $y = X'\\beta + \\epsilon$ to derive the value of the weight vector $\\beta$  in terms of the observed data. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    y &= X' \\beta  + \\epsilon \\\\\n",
    "    \\epsilon &= y - X' \\beta \\\\\n",
    "    \\epsilon'\\epsilon &= y'y - 2X'y \\beta + \\beta'X'X \\beta \\\\\n",
    "    \\frac{\\partial}{\\partial \\beta} \\epsilon'\\epsilon &= \\frac{\\partial}{\\partial \\beta}y'y - 2X'y \\beta + \\beta'X'X \\beta \\\\\n",
    "    0 &= -2X'y + 2X'X \\beta \\\\\n",
    "    2X'X \\beta &= 2X'y \\\\ \n",
    "    \\beta &= (X'X)^{-1}X'y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The last line is the OLS estimator of the extent to which the regressors $X$ covary with the dependent data $y$ on average. We can use this definition to directly to compute our weight vector. In `fit_exec()` below, the variable `cov` (for covariance) captures the numerator ($X'Y$) and the variable `var_design` (for variance of the design matrix) captures the denominator ($X'X$).\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class OLS:\n",
    "\n",
    "    @staticmethod\n",
    "    def fit(formula: str, data: xr.Dataset) -> RegModel:\n",
    "        y: DesignMatrix\n",
    "        x: DesignMatrix\n",
    "        y, x = dmatrices(formula, data)\n",
    "        y_tensor: tf.Tensor = tf.constant(y)\n",
    "        x_tensor: tf.Tensor = tf.constant(x)\n",
    "        betas: tf.Tensor = OLS.fit_exec(y_tensor, x_tensor)\n",
    "        out: RegModel = RegModel(data, y, x, betas, \"Direct Ordinary Least Squares\")\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_exec(y: tf.Tensor, x: tf.Tensor) -> tf.Tensor:\n",
    "        cov: tf.Tensor = tf.linalg.matmul(tf.transpose(x), y)\n",
    "        var_design: tf.Tensor = tf.linalg.matmul(tf.transpose(x), x)\n",
    "        inv_var_design: tf.Tensor = tf.linalg.inv(var_design)\n",
    "        betas: tf.Tensor = tf.matmul(inv_var_design, cov)\n",
    "        return betas\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "analytic: RegModel = OLS.fit(\"demeaned_mpg ~ demeaned_weight\", ds)\n",
    "    \n",
    "analytic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f4117-ecd4-41cf-bbeb-2baddf0d5de5",
   "metadata": {},
   "source": [
    "As can be seen, our intercept is effectively zero (appropriate since we demeaned the data) and the slope is about -0.008. In other words, on average, cars that are one pound above average weight are associated with being able to get -0.008 fewer miles to the gallon. Equivalently, cars that are about 130 pounds above average weight tend to get one mile less per gallon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3b06c-79b2-4976-8739-cd7e33404339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_est(betas: tf.Tensor, support: tf.Tensor, ax: Axes) -> Axes:\n",
    "    map_func: Callable[[float], float] = lambda x: betas[0] + betas[1] * x\n",
    "    ys: tf.Tensor = tf.map_fn(fn=map_func, elems=support)\n",
    "    ax.plot(support, ys, linestyle=\"--\", color=\"grey\")\n",
    "    return ax\n",
    "    \n",
    "plot_est(\n",
    "    betas=analytic.betas,\n",
    "    support=tf.linspace(ds[\"demeaned_weight\"].min().data, ds[\"demeaned_weight\"].max().data, 100),\n",
    "    ax=plot_data(ds, \"demeaned_weight\", \"demeaned_mpg\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3d39f-438f-4429-b296-19e497056b45",
   "metadata": {},
   "source": [
    "Given our estimate of the weight vector $\\beta$ we can compute the variance in the errors (the gap between our estimate and observed data). This variance is our estimate of the variance in $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d20e087-0045-4a09-a694-6aee3c5520d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_std_dev(beta: tf.Tensor, y: tf.Tensor, x: tf.Tensor) -> tf.Tensor:\n",
    "    # Apply the weight parameters to the data\n",
    "    apply_weights: Callable[[float], float] = lambda datum: beta[0] + beta[1] * datum\n",
    "    weighted_data: tf.Tensor = tf.map_fn(apply_weights, x)\n",
    "        \n",
    "    # Calculate variance in errors (note the avg is zero)\n",
    "    errors: tf.Tensor = y - weighted_data\n",
    "    variance: tf.Tensor = tf.reduce_mean (\n",
    "        tf.pow(errors, 2)\n",
    "    )\n",
    "    return tf.pow(variance, 0.5)\n",
    "        \n",
    "compute_std_dev(\n",
    "    beta=analytic.betas,\n",
    "    y=tf.constant(ds[\"demeaned_mpg\"].data),\n",
    "    x=tf.constant(ds[\"demeaned_weight\"].data)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f11ae90-04dc-415d-9286-9fffc99d549d",
   "metadata": {},
   "source": [
    "As can be seen, the variance in $\\epsilon$ is roughly 10.15."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62f110-c964-4408-867a-a51c94bb2778",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Solution\n",
    "\n",
    "When estimating by maximum likelihood, our approach shifts a bit. We don't focus on analytically solving for $\\beta$, but rather ask ourselves the following:\n",
    "\n",
    "> Of all the values that $\\beta$ could take, which value is most consistent with the data we observe?\n",
    "\n",
    "To answer this question, we need to know how plausible the data are given some value for $\\beta$. This concept is known as statistical likelihood. Once we can evaluate the likelihood for some value of $\\beta$, we can do for all (relevant) values of $\\beta$. After doing so, our estimate is just the most plausible value (i.e. the maximum likelihood estimate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60750f8a-7618-42ce-9793-697d12e850cd",
   "metadata": {},
   "source": [
    "## How do we determine the likelihood at a given value of $\\beta$?\n",
    "\n",
    "Suppose we had some random variable that was distributed normally with zero mean and a standard deviation of one (i.e. $x \\sim N(0, 1)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e3e20-c78b-4119-874f-51ddfdfbd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_norm: tfd.Normal = tfd.Normal(loc=0., scale=1.)\n",
    "    \n",
    "def plot_univ(xs: tf.Tensor, figsize: Tuple[int, int] = (15, 10), **kwargs) -> Axes:\n",
    "    fig: Figure\n",
    "    ax: Axes\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sb.kdeplot(xs, ax=ax)\n",
    "    return ax\n",
    "    \n",
    "    \n",
    "plot_univ(std_norm.sample(10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf932c7d-bbb3-4e30-9916-2de4d9db0acf",
   "metadata": {},
   "source": [
    "Given that we know the parameters that define the distribution of the random variable (`loc` is $\\mu$ and `scale` is $\\sigma$), we can ask the following:\n",
    "\n",
    "> Given that this random variable is distributed normally with with our values for $\\mu$ and $\\sigma$, if we were to take a sample, what is the likelihood that the sampled value would be 0? 1? -2?\n",
    "\n",
    "To find this out, we need only evaluate the probability density function for our distribution at those values. The value yielded is a relative measure of sampling plausibility (i.e. the likelihood). For consistency and computational reasons that will later become relevant, we'll use the logged likelihood value instead of the untransformed likelihood. Since logging is a [monotonic transformation](https://en.wikipedia.org/wiki/Monotonic_function) the ordering is preserved, so we still have the relative relationships we are seeking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004b0f18-64c4-4c23-99c5-7eac924d3b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ll_values(dist: tfd.Distribution, values: tf.Tensor, ax: Axes, lab_y: float = 0.35, lab_dx: float = 0.1) -> Axes:\n",
    "    with_log_lik: Tuple[float, float] = [(orig.numpy(), dist.log_prob(orig).numpy()) for orig in values]\n",
    "    for orig, ll in with_log_lik:\n",
    "        ax.axvline(x=orig, color=\"orange\")\n",
    "        ax.annotate(\"{:.3f}\".format(ll), xy=(orig - lab_dx, lab_y), rotation=90)\n",
    "    return ax\n",
    "\n",
    "plot_ll_values(\n",
    "    dist=std_norm,\n",
    "    values=tf.constant([0., 1., -2.]),\n",
    "    ax=plot_univ(std_norm.sample(10000))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77897107-41ad-4cae-9ecb-a19f776bcd25",
   "metadata": {},
   "source": [
    "As can be seen, and should accord with intuition, 0 is the value that is most plausibly drawn from the distribution. It has the least negative log likelihood. Drawing 1 is less likely, and drawing -2 is less likely still.\n",
    "\n",
    "So what does this have to do with $\\beta$? Recall our regression equation:\n",
    "\n",
    "$$y = X'\\beta + \\epsilon$$\n",
    "\n",
    "One of the fundamental assumptions that underlies OLS is that our error term, $\\epsilon$ is normally distributed with mean zero and some positive variance (i.e. $\\epsilon \\sim N(0, \\sigma)$). If $\\epsilon$ does not add anything on average, then the average value of $y$ must come from $X'\\beta$. Moreover, $X'\\beta$ is a computed value, not a random variable. All of the variation on the right side is coming from that $\\sigma$ in the distribution of $\\epsilon$. Consequently, a different way to express the regression equation is as follows:\n",
    "\n",
    "$$y \\sim N(X'\\beta, \\sigma)$$\n",
    "\n",
    "Written in this form, a couple things become clear:\n",
    "\n",
    "1. We now have distributions that we can evaluate at several points (specifically those coming from the data $X$).\n",
    "2. We can set values for the parameters $\\beta$ and $\\sigma$ to *define* specific instances of those distributions.\n",
    "\n",
    "We are doing the same thing we did with our standard normal example, but now we are using our parameters to calculate `loc` and directly define `scale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38a0af-5b4c-4edf-acf7-e0999428eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_ll(beta: tf.Tensor, sigma: float, y: tf.Tensor, x: tf.Tensor) -> tf.Tensor:\n",
    "    # Apply the weight parameters to the data (testing the plausibility of these)\n",
    "    apply_weights: Callable[[float], float] = lambda datum: beta[0] + beta[1] * datum\n",
    "    weighted_data: tf.Tensor = tf.map_fn(apply_weights, x)\n",
    "        \n",
    "    # Define the distribution using the expected value of the observed response data and sigma\n",
    "    avg_data: float = tf.reduce_mean(y)\n",
    "    dist: tfd.Normal = tfd.Normal(avg_data, sigma)\n",
    "        \n",
    "    # Compute the log likelihood\n",
    "    data_lls: tf.Tensor = tf.map_fn(dist.log_prob, weighted_data)\n",
    "    total_ll: tf.Tensor = tf.reduce_sum(data_lls)\n",
    "    return total_ll\n",
    "\n",
    "@dataclass\n",
    "class SimpleParams:\n",
    "    beta0: float\n",
    "    beta1: float\n",
    "    sigma: float\n",
    "        \n",
    "params: List[SimpleParams] = [\n",
    "    SimpleParams(0., -0.008, 10.15),\n",
    "    SimpleParams(0., -0.008, 7.8),\n",
    "    SimpleParams(0., -0.008, 100.15),\n",
    "    SimpleParams(100., -0.008, 10.15),\n",
    "    SimpleParams(0., 1, 10.15),\n",
    "]\n",
    "\n",
    "for p in params:\n",
    "    print(p)\n",
    "    print(ols_ll(\n",
    "        beta=tf.constant([p.beta0, p.beta1], dtype=tf.float64),\n",
    "        sigma=p.sigma,\n",
    "        y=tf.constant(ds[\"demeaned_mpg\"].data),\n",
    "        x=tf.constant(ds[\"demeaned_weight\"].data)\n",
    "    ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d13eaeb-9b94-4510-96ed-3877de1cc1aa",
   "metadata": {},
   "source": [
    "As can be seen choosing parameter values close to what we estimated analytically yields the least negative number. Clearly these are just spot checks, but really it's foreshadowing the eventual result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9ec486-7424-4787-99ed-9da1905a9041",
   "metadata": {},
   "source": [
    "## How do we determine the maximum likelihood estimate of $\\beta$?\n",
    "\n",
    "One way to determine the MLE is to create a mesh. In this case, it would be three dimensions. Along one axis of the cube, we'd have a bunch of values for the intercept $\\beta_0$. Along another we'd have a bunch of values for $\\beta_1$, and along the last a bunch of values for $\\sigma$. The intersections of all the values in the cube could then be used to calculate the likelihoods given the data. The intersection with the highest likelihood wins and we take the values of the parameters at that intersection as our MLE.\n",
    "\n",
    "The thing is, that can get pretty computationally intensive depending on how many parameters we have and how fine grained we want the mesh to be. A more efficient solution would be to represent variation in our parameters as a likelihood function. If we can do that, we can differentiate that function and use the output to determine the maximum value. This turns out to be much more efficient and one of the great selling points for Tensorflow is its automatic differentiation capability. In other words, we can get Tensorflow to the heavy lifting on the calculus front.\n",
    "\n",
    "All that said, let's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ddc73-4ff3-45f8-abe2-5e60bef2b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "y: tf.Tensor = tf.constant(ds[\"demeaned_mpg\"].data)\n",
    "x: tf.Tensor = tf.concat([\n",
    "    tf.ones((len(y), 1), dtype=tf.float64), \n",
    "    tf.reshape(ds[\"demeaned_weight\"].data, shape=(len(y), 1))\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32f307-a8b9-46e1-ac96-680cb83cca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OLSmle:\n",
    "    y_arr: xr.DataArray\n",
    "    x_arr: xr.DataArray\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        self.y: tf.Tensor = tf.constant(self.y_arr.data)\n",
    "        self.x: tf.Tensor = tf.concat([\n",
    "            tf.ones((len(y), 1), dtype=tf.float64), \n",
    "            tf.reshape(self.x_arr.data, shape=(len(y), 1))\n",
    "        ], axis=1)\n",
    "    \n",
    "    def neg_ols_ll(self, params: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Requires params vector to end with sigma\"\"\"\n",
    "        beta_len: int = params.shape[0] - 1\n",
    "        beta: tf.Tensor = tf.cast(tf.reshape(params[:-1], (beta_len, 1)), dtype=tf.float64)\n",
    "        sigma: tf.Tensor = tf.cast(params[-1], dtype=tf.float64)\n",
    "        mu: tf.Tensor = tf.matmul(self.x, beta)\n",
    "#         print(beta_len)\n",
    "#         print(beta)\n",
    "#         print(sigma)\n",
    "#         print(mu)\n",
    "        return -tf.reduce_sum(tfd.Normal(tf.reduce_mean(self.y), sigma).log_prob(mu))\n",
    "\n",
    "    def neg_ols_ll_surface(\n",
    "        self,\n",
    "        beta0_vals: tf.Tensor,\n",
    "        beta1_vals: tf.Tensor,\n",
    "        sigma_vals: tf.Tensor\n",
    "    ) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        There must be a better way to do this, but not sure how to get\n",
    "        around the need for conformant matrix multiplication (beta against data)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(beta0_vals)\n",
    "        print(beta1_vals)\n",
    "        print(sigma_vals)\n",
    "        out: tf.Tensor = tf.zeros(shape=(\n",
    "            beta0_vals.shape[0],\n",
    "            beta1_vals.shape[0],\n",
    "            sigma_vals.shape[0]\n",
    "        ))\n",
    "        \n",
    "        for i, b0 in enumerate(beta0_vals):\n",
    "            for j, b1 in enumerate(beta1_vals):\n",
    "                for k, s in enumerate(sigma_vals):\n",
    "                    print(b0, b1, s)\n",
    "                    ps: tf.Tensor = tf.constant([b0.numpy(), b1.numpy(), s.numpy()])\n",
    "                    print(ps)\n",
    "                    out[i, j, k] = self.neg_ols_ll(params=ps) #breaks on index assignment\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def neg_ols_ll_w_gradient(self, params: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Tensorflow will automatically compute the partial derivatives\n",
    "        associated with out log-likelihood function we have deployed. We\n",
    "        can get the gradients for each parameter returned to us when\n",
    "        we compute the negative log likelihood.\n",
    "        \"\"\"\n",
    "        return tfp.math.value_and_gradient(self.neg_ols_ll, params)\n",
    "    \n",
    "    def fit(\n",
    "        self, \n",
    "        init_param_vals: tf.Tensor = tf.constant([0., 0., 1.], dtype=tf.float64),\n",
    "        tol: float = 1e-8,\n",
    "        **kwargs\n",
    "    ) -> tf.Tensor:\n",
    "        opti_results: tf.Tensor = tfp.optimizer.bfgs_minimize(\n",
    "            value_and_gradients_function=self.neg_ols_ll_w_gradient,\n",
    "            initial_position=init_param_vals,\n",
    "            tolerance=tol,\n",
    "            **kwargs\n",
    "        )\n",
    "        status: str = f\"\"\"\n",
    "        RESULTS\n",
    "        Converged: {opti_results.converged.numpy()}\n",
    "        Location of Minimum: {opti_results.position.numpy()}\n",
    "        Number of Iterations: {opti_results.num_iterations.numpy()}\n",
    "        \"\"\"\n",
    "        print(status)\n",
    "        return opti_results.position.numpy()\n",
    "        \n",
    "    \n",
    "mle: OLSmle = OLSmle(\n",
    "    y_arr=ds[\"demeaned_mpg\"],\n",
    "    x_arr=ds[\"demeaned_weight\"]\n",
    ")\n",
    "    \n",
    "mle.neg_ols_ll_w_gradient(params=tf.constant([0., -0.8, 10.15], dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b543c7e-db7b-4682-991d-7b1a4cc38780",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-10, 10):\n",
    "    print(i, mle.neg_ols_ll(params=tf.constant([float(i), -0.008, 10.15], dtype=tf.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2a8d7-ec5a-430f-8ed5-d44b60860a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-10, 10):\n",
    "    print(i, mle.neg_ols_ll(params=tf.constant([0., float(i), 10.15], dtype=tf.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a0c0f-fb1b-4cf8-ae98-18106ca596c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-10, 10):\n",
    "    print(i, mle.neg_ols_ll(params=tf.constant([0., -0.008, float(i)], dtype=tf.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9053d2-7046-40f7-b490-6fd995b77d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mle.neg_ols_ll_w_gradient(mle.fit(init_param_vals=tf.constant([1., -1., 8.15], dtype=tf.float64), tol=1e-15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114656b5-b884-420b-91ea-92c6a4a92c2c",
   "metadata": {},
   "source": [
    "# Bayesian Solution\n",
    "\n",
    "The bayesian approach to estimation overlaps with the MLE approach to the extent that there is still heavy reliance on the likelihood function. The difference now will be that the likelihood serves the purpose of allowing data to update our prior beliefs about our parameter values. Instead of making no claims about prior distributions (i.e. implicitly assuming all values are equally plausible), we will directly model our paramsters as random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96cccdd-1130-4ad5-95be-47180bbf7d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_likelihood(beta0: tf.Tensor, beta1: tf.Tensor, sigma: tf.Tensor) -> tfd.Independent:\n",
    "    beta = tf.concat([\n",
    "        tf.reshape(beta0, shape=(1, 1)),\n",
    "        tf.reshape(beta1, shape=(1, 1))\n",
    "    ], axis=0)\n",
    "    out: tfd.Independent = tfd.Independent(tfd.Normal(\n",
    "        loc=tf.matmul(x, beta),\n",
    "        scale=sigma\n",
    "    ), reinterpreted_batch_ndims=1)\n",
    "    return out\n",
    "    \n",
    "\n",
    "model_ols: tfd.JointDistributionNamed = tfd.JointDistributionNamed(dict(\n",
    "    beta0 = tfd.Normal(loc=tf.cast(0., dtype=tf.float64), scale=1.),\n",
    "    beta1 = tfd.Normal(loc=tf.cast(0., dtype=tf.float64), scale=0.01),\n",
    "    sigma = tfd.Uniform(low=tf.cast(7., dtype=tf.float64), high=8.),\n",
    "    likelihood = bayes_likelihood\n",
    "))\n",
    "    \n",
    "model_ols.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7196c958-e57e-46a5-a4e2-3ca9d7fb437b",
   "metadata": {},
   "source": [
    "This `JointDistribution` formulation allows us to directly sample from our composite prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb3d4c5-4426-4585-8711-7d32a266a630",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ols.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a357203-35de-4af9-93c9-d358bc0f4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(model_ols.log_prob({\n",
    "    \"sigma\": 7.,\n",
    "    \"beta0\": 0.,\n",
    "    \"beta1\": -0.008,\n",
    "    \"y\": y\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eae878-b818-4449-864c-56fbd1d6729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_log_prob_fn(beta0: float, beta1: float, sigma: float) -> tf.Tensor:\n",
    "    params: Dict[str, float] = dict(\n",
    "        beta0=beta0,\n",
    "        beta1=beta1,\n",
    "        sigma=sigma,\n",
    "        y=y\n",
    "    )\n",
    "    print({k: v for k, v in params.items() if k != \"y\"})\n",
    "    out: tf.Tensor = tf.reduce_sum(model_ols.log_prob(params))\n",
    "    return out\n",
    "\n",
    "# target_log_prob_fn(0., -0.008, 7.86)\n",
    "num_results: int = int(1e2)\n",
    "num_burn_in_steps: int = int(1e1)\n",
    "\n",
    "hmc_kernel: tfp.mcmc.HamiltonianMonteCarlo = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "    target_log_prob_fn=target_log_prob_fn,\n",
    "    step_size=1.0,\n",
    "    num_leapfrog_steps=3\n",
    ")\n",
    "    \n",
    "adaptive_hmc_kernel: tfp.mcmc.SimpleStepSizeAdaptation = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "    inner_kernel=hmc_kernel,\n",
    "    num_adaptation_steps=int(num_burn_in_steps * 0.8)\n",
    ")\n",
    "    \n",
    "mh_kernel: tfp.mcmc.MetropolisHastings = tfp.mcmc.MetropolisHastings(\n",
    "    tfp.mcmc.UncalibratedHamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=target_log_prob_fn,\n",
    "        step_size=0.1,\n",
    "        num_leapfrog_steps=3\n",
    "    )\n",
    ")\n",
    "    \n",
    "@tf.function\n",
    "def run_chain():\n",
    "    \n",
    "    samples, is_accepted = tfp.mcmc.sample_chain(\n",
    "        num_results = num_results,\n",
    "        current_state=[\n",
    "            tf.constant([0.], dtype=tf.float64),\n",
    "            tf.constant([1.], dtype=tf.float64),\n",
    "            tf.constant([7.], dtype=tf.float64),\n",
    "        ],\n",
    "        num_burnin_steps=num_burn_in_steps,\n",
    "        kernel=adaptive_hmc_kernel,\n",
    "        trace_fn=lambda _, pkr: pkr.inner_results.is_accepted\n",
    "    )\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d186db-ce3c-4de2-bf4b-b848dbe7fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b5c34c-1469-4203-8335-8605c63ef612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
